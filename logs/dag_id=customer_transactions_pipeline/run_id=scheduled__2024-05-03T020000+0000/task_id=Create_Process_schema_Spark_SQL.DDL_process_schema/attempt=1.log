[2024-05-13T04:46:55.742+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-13T04:46:55.760+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: customer_transactions_pipeline.Create_Process_schema_Spark_SQL.DDL_process_schema scheduled__2024-05-03T02:00:00+00:00 [queued]>
[2024-05-13T04:46:55.769+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: customer_transactions_pipeline.Create_Process_schema_Spark_SQL.DDL_process_schema scheduled__2024-05-03T02:00:00+00:00 [queued]>
[2024-05-13T04:46:55.770+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2024-05-13T04:46:55.784+0000] {taskinstance.py:2327} INFO - Executing <Task(DatabricksRunNowOperator): Create_Process_schema_Spark_SQL.DDL_process_schema> on 2024-05-03 02:00:00+00:00
[2024-05-13T04:46:55.788+0000] {standard_task_runner.py:63} INFO - Started process 12509 to run task
[2024-05-13T04:46:55.791+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'customer_transactions_pipeline', 'Create_Process_schema_Spark_SQL.DDL_process_schema', 'scheduled__2024-05-03T02:00:00+00:00', '--job-id', '68', '--raw', '--subdir', 'DAGS_FOLDER/dag.py', '--cfg-path', '/tmp/tmpqmd59jhe']
[2024-05-13T04:46:55.793+0000] {standard_task_runner.py:91} INFO - Job 68: Subtask Create_Process_schema_Spark_SQL.DDL_process_schema
[2024-05-13T04:46:55.842+0000] {task_command.py:426} INFO - Running <TaskInstance: customer_transactions_pipeline.Create_Process_schema_Spark_SQL.DDL_process_schema scheduled__2024-05-03T02:00:00+00:00 [running]> on host f5da772b0226
[2024-05-13T04:46:55.925+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='mm22@gmail.com' AIRFLOW_CTX_DAG_OWNER='ct' AIRFLOW_CTX_DAG_ID='customer_transactions_pipeline' AIRFLOW_CTX_TASK_ID='Create_Process_schema_Spark_SQL.DDL_process_schema' AIRFLOW_CTX_EXECUTION_DATE='2024-05-03T02:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-05-03T02:00:00+00:00'
[2024-05-13T04:46:55.926+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-13T04:46:55.948+0000] {base.py:84} INFO - Using connection ID 'databricks_default' for task execution.
[2024-05-13T04:46:55.950+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:46:56.494+0000] {databricks.py:56} INFO - Run submitted with run_id: 1115770323445064
[2024-05-13T04:46:56.495+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:46:56.814+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:46:57.119+0000] {databricks.py:109} INFO - Create_Process_schema_Spark_SQL.DDL_process_schema in run state: {'life_cycle_state': 'QUEUED', 'result_state': '', 'state_message': ''}
[2024-05-13T04:46:57.120+0000] {databricks.py:110} INFO - View run status, Spark UI, and logs at https://adb-321042217431130.10.azuredatabricks.net/?o=321042217431130#job/243865987601161/run/1115770323445064
[2024-05-13T04:46:57.121+0000] {databricks.py:111} INFO - Sleeping for 30 seconds.
[2024-05-13T04:47:27.152+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:47:27.477+0000] {databricks.py:109} INFO - Create_Process_schema_Spark_SQL.DDL_process_schema in run state: {'life_cycle_state': 'QUEUED', 'result_state': '', 'state_message': ''}
[2024-05-13T04:47:27.478+0000] {databricks.py:110} INFO - View run status, Spark UI, and logs at https://adb-321042217431130.10.azuredatabricks.net/?o=321042217431130#job/243865987601161/run/1115770323445064
[2024-05-13T04:47:27.479+0000] {databricks.py:111} INFO - Sleeping for 30 seconds.
[2024-05-13T04:47:57.510+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:47:57.854+0000] {databricks.py:67} INFO - Create_Process_schema_Spark_SQL.DDL_process_schema completed successfully.
[2024-05-13T04:47:57.855+0000] {databricks.py:68} INFO - View run status, Spark UI, and logs at https://adb-321042217431130.10.azuredatabricks.net/?o=321042217431130#job/243865987601161/run/1115770323445064
[2024-05-13T04:47:57.856+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-13T04:47:57.865+0000] {taskinstance.py:1205} INFO - Marking task as SUCCESS. dag_id=customer_transactions_pipeline, task_id=Create_Process_schema_Spark_SQL.DDL_process_schema, execution_date=20240503T020000, start_date=20240513T044655, end_date=20240513T044757
[2024-05-13T04:47:57.917+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-05-13T04:47:57.947+0000] {taskinstance.py:3482} INFO - 3 downstream tasks scheduled from follow-on schedule check
[2024-05-13T04:47:57.950+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-05-13T04:56:22.315+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-13T04:56:22.334+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: customer_transactions_pipeline.Create_Process_schema_Spark_SQL.DDL_process_schema scheduled__2024-05-03T02:00:00+00:00 [queued]>
[2024-05-13T04:56:22.342+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: customer_transactions_pipeline.Create_Process_schema_Spark_SQL.DDL_process_schema scheduled__2024-05-03T02:00:00+00:00 [queued]>
[2024-05-13T04:56:22.343+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2024-05-13T04:56:22.356+0000] {taskinstance.py:2327} INFO - Executing <Task(DatabricksRunNowOperator): Create_Process_schema_Spark_SQL.DDL_process_schema> on 2024-05-03 02:00:00+00:00
[2024-05-13T04:56:22.360+0000] {standard_task_runner.py:63} INFO - Started process 12756 to run task
[2024-05-13T04:56:22.363+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'customer_transactions_pipeline', 'Create_Process_schema_Spark_SQL.DDL_process_schema', 'scheduled__2024-05-03T02:00:00+00:00', '--job-id', '87', '--raw', '--subdir', 'DAGS_FOLDER/dag.py', '--cfg-path', '/tmp/tmpl2drhxon']
[2024-05-13T04:56:22.365+0000] {standard_task_runner.py:91} INFO - Job 87: Subtask Create_Process_schema_Spark_SQL.DDL_process_schema
[2024-05-13T04:56:22.414+0000] {task_command.py:426} INFO - Running <TaskInstance: customer_transactions_pipeline.Create_Process_schema_Spark_SQL.DDL_process_schema scheduled__2024-05-03T02:00:00+00:00 [running]> on host f5da772b0226
[2024-05-13T04:56:22.501+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='mm22@gmail.com' AIRFLOW_CTX_DAG_OWNER='ct' AIRFLOW_CTX_DAG_ID='customer_transactions_pipeline' AIRFLOW_CTX_TASK_ID='Create_Process_schema_Spark_SQL.DDL_process_schema' AIRFLOW_CTX_EXECUTION_DATE='2024-05-03T02:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-05-03T02:00:00+00:00'
[2024-05-13T04:56:22.502+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-13T04:56:22.524+0000] {base.py:84} INFO - Using connection ID 'databricks_default' for task execution.
[2024-05-13T04:56:22.526+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:56:22.933+0000] {databricks.py:56} INFO - Run submitted with run_id: 812127795248400
[2024-05-13T04:56:22.933+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:56:23.385+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:56:23.712+0000] {databricks.py:109} INFO - Create_Process_schema_Spark_SQL.DDL_process_schema in run state: {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': ''}
[2024-05-13T04:56:23.713+0000] {databricks.py:110} INFO - View run status, Spark UI, and logs at https://adb-321042217431130.10.azuredatabricks.net/?o=321042217431130#job/243865987601161/run/812127795248400
[2024-05-13T04:56:23.714+0000] {databricks.py:111} INFO - Sleeping for 30 seconds.
[2024-05-13T04:56:53.715+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:56:54.080+0000] {databricks.py:67} INFO - Create_Process_schema_Spark_SQL.DDL_process_schema completed successfully.
[2024-05-13T04:56:54.081+0000] {databricks.py:68} INFO - View run status, Spark UI, and logs at https://adb-321042217431130.10.azuredatabricks.net/?o=321042217431130#job/243865987601161/run/812127795248400
[2024-05-13T04:56:54.082+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-13T04:56:54.091+0000] {taskinstance.py:1205} INFO - Marking task as SUCCESS. dag_id=customer_transactions_pipeline, task_id=Create_Process_schema_Spark_SQL.DDL_process_schema, execution_date=20240503T020000, start_date=20240513T045622, end_date=20240513T045654
[2024-05-13T04:56:54.139+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-05-13T04:56:54.168+0000] {taskinstance.py:3482} INFO - 3 downstream tasks scheduled from follow-on schedule check
[2024-05-13T04:56:54.170+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-05-13T06:51:00.939+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-13T06:51:00.958+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: customer_transactions_pipeline.Create_Process_schema_Spark_SQL.DDL_process_schema scheduled__2024-05-03T02:00:00+00:00 [queued]>
[2024-05-13T06:51:00.965+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: customer_transactions_pipeline.Create_Process_schema_Spark_SQL.DDL_process_schema scheduled__2024-05-03T02:00:00+00:00 [queued]>
[2024-05-13T06:51:00.966+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2024-05-13T06:51:00.983+0000] {taskinstance.py:2327} INFO - Executing <Task(DatabricksRunNowOperator): Create_Process_schema_Spark_SQL.DDL_process_schema> on 2024-05-03 02:00:00+00:00
[2024-05-13T06:51:00.990+0000] {standard_task_runner.py:63} INFO - Started process 15310 to run task
[2024-05-13T06:51:00.993+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'customer_transactions_pipeline', 'Create_Process_schema_Spark_SQL.DDL_process_schema', 'scheduled__2024-05-03T02:00:00+00:00', '--job-id', '109', '--raw', '--subdir', 'DAGS_FOLDER/dag.py', '--cfg-path', '/tmp/tmpm4lap0ua']
[2024-05-13T06:51:00.995+0000] {standard_task_runner.py:91} INFO - Job 109: Subtask Create_Process_schema_Spark_SQL.DDL_process_schema
[2024-05-13T06:51:01.048+0000] {task_command.py:426} INFO - Running <TaskInstance: customer_transactions_pipeline.Create_Process_schema_Spark_SQL.DDL_process_schema scheduled__2024-05-03T02:00:00+00:00 [running]> on host f5da772b0226
[2024-05-13T06:51:01.135+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='mm22@gmail.com' AIRFLOW_CTX_DAG_OWNER='ct' AIRFLOW_CTX_DAG_ID='customer_transactions_pipeline' AIRFLOW_CTX_TASK_ID='Create_Process_schema_Spark_SQL.DDL_process_schema' AIRFLOW_CTX_EXECUTION_DATE='2024-05-03T02:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-05-03T02:00:00+00:00'
[2024-05-13T06:51:01.136+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-13T06:51:01.159+0000] {base.py:84} INFO - Using connection ID 'databricks_default' for task execution.
[2024-05-13T06:51:01.160+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T06:51:01.590+0000] {databricks.py:56} INFO - Run submitted with run_id: 556291361652134
[2024-05-13T06:51:01.592+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T06:51:01.986+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T06:51:02.322+0000] {databricks.py:109} INFO - Create_Process_schema_Spark_SQL.DDL_process_schema in run state: {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': ''}
[2024-05-13T06:51:02.324+0000] {databricks.py:110} INFO - View run status, Spark UI, and logs at https://adb-321042217431130.10.azuredatabricks.net/?o=321042217431130#job/243865987601161/run/556291361652134
[2024-05-13T06:51:02.326+0000] {databricks.py:111} INFO - Sleeping for 30 seconds.
[2024-05-13T06:51:32.331+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T06:51:32.664+0000] {databricks.py:67} INFO - Create_Process_schema_Spark_SQL.DDL_process_schema completed successfully.
[2024-05-13T06:51:32.666+0000] {databricks.py:68} INFO - View run status, Spark UI, and logs at https://adb-321042217431130.10.azuredatabricks.net/?o=321042217431130#job/243865987601161/run/556291361652134
[2024-05-13T06:51:32.667+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-13T06:51:32.687+0000] {taskinstance.py:1205} INFO - Marking task as SUCCESS. dag_id=customer_transactions_pipeline, task_id=Create_Process_schema_Spark_SQL.DDL_process_schema, execution_date=20240503T020000, start_date=20240513T065100, end_date=20240513T065132
[2024-05-13T06:51:32.733+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-05-13T06:51:32.783+0000] {taskinstance.py:3482} INFO - 3 downstream tasks scheduled from follow-on schedule check
[2024-05-13T06:51:32.785+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
