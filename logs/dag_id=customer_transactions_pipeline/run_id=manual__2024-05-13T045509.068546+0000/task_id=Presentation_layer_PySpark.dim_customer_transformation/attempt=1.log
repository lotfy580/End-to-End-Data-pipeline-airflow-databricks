[2024-05-13T04:58:29.656+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-13T04:58:29.680+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: customer_transactions_pipeline.Presentation_layer_PySpark.dim_customer_transformation manual__2024-05-13T04:55:09.068546+00:00 [queued]>
[2024-05-13T04:58:29.689+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: customer_transactions_pipeline.Presentation_layer_PySpark.dim_customer_transformation manual__2024-05-13T04:55:09.068546+00:00 [queued]>
[2024-05-13T04:58:29.690+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2024-05-13T04:58:29.710+0000] {taskinstance.py:2327} INFO - Executing <Task(DatabricksRunNowOperator): Presentation_layer_PySpark.dim_customer_transformation> on 2024-05-13 04:55:09.068546+00:00
[2024-05-13T04:58:29.714+0000] {standard_task_runner.py:63} INFO - Started process 12830 to run task
[2024-05-13T04:58:29.717+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'customer_transactions_pipeline', 'Presentation_layer_PySpark.dim_customer_transformation', 'manual__2024-05-13T04:55:09.068546+00:00', '--job-id', '102', '--raw', '--subdir', 'DAGS_FOLDER/dag.py', '--cfg-path', '/tmp/tmpcel96ly9']
[2024-05-13T04:58:29.719+0000] {standard_task_runner.py:91} INFO - Job 102: Subtask Presentation_layer_PySpark.dim_customer_transformation
[2024-05-13T04:58:29.768+0000] {task_command.py:426} INFO - Running <TaskInstance: customer_transactions_pipeline.Presentation_layer_PySpark.dim_customer_transformation manual__2024-05-13T04:55:09.068546+00:00 [running]> on host f5da772b0226
[2024-05-13T04:58:29.853+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='mm22@gmail.com' AIRFLOW_CTX_DAG_OWNER='ct' AIRFLOW_CTX_DAG_ID='customer_transactions_pipeline' AIRFLOW_CTX_TASK_ID='Presentation_layer_PySpark.dim_customer_transformation' AIRFLOW_CTX_EXECUTION_DATE='2024-05-13T04:55:09.068546+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-05-13T04:55:09.068546+00:00'
[2024-05-13T04:58:29.854+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-13T04:58:29.878+0000] {base.py:84} INFO - Using connection ID 'databricks_default' for task execution.
[2024-05-13T04:58:29.879+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:58:30.400+0000] {databricks.py:56} INFO - Run submitted with run_id: 276803408765040
[2024-05-13T04:58:30.400+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:58:30.884+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:58:31.325+0000] {databricks.py:109} INFO - Presentation_layer_PySpark.dim_customer_transformation in run state: {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': ''}
[2024-05-13T04:58:31.326+0000] {databricks.py:110} INFO - View run status, Spark UI, and logs at https://adb-321042217431130.10.azuredatabricks.net/?o=321042217431130#job/1082082529585351/run/276803408765040
[2024-05-13T04:58:31.327+0000] {databricks.py:111} INFO - Sleeping for 30 seconds.
[2024-05-13T04:59:01.343+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:59:01.704+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:59:02.232+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-13T04:59:02.261+0000] {taskinstance.py:2890} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/databricks/operators/databricks.py", line 858, in execute
    _handle_databricks_operator_execution(self, hook, self.log, context)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/databricks/operators/databricks.py", line 107, in _handle_databricks_operator_execution
    raise AirflowException(error_message)
airflow.exceptions.AirflowException: Presentation_layer_PySpark.dim_customer_transformation failed with terminal state: {'life_cycle_state': 'INTERNAL_ERROR', 'result_state': 'FAILED', 'state_message': 'Task dim_customer_transformation failed with message: Workload failed, see run output for details. This caused all downstream tasks to get skipped.'} and with the error [INCOMPATIBLE_COLUMN_TYPE] UNION can only be performed on tables with compatible column types. The 7th column of the second table is "BOOLEAN" type which is not compatible with "STRING" at the same column of the first table.. SQLSTATE: 42825
[2024-05-13T04:59:02.273+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=customer_transactions_pipeline, task_id=Presentation_layer_PySpark.dim_customer_transformation, execution_date=20240513T045509, start_date=20240513T045829, end_date=20240513T045902
[2024-05-13T04:59:02.306+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 102 for task Presentation_layer_PySpark.dim_customer_transformation (Presentation_layer_PySpark.dim_customer_transformation failed with terminal state: {'life_cycle_state': 'INTERNAL_ERROR', 'result_state': 'FAILED', 'state_message': 'Task dim_customer_transformation failed with message: Workload failed, see run output for details. This caused all downstream tasks to get skipped.'} and with the error [INCOMPATIBLE_COLUMN_TYPE] UNION can only be performed on tables with compatible column types. The 7th column of the second table is "BOOLEAN" type which is not compatible with "STRING" at the same column of the first table.. SQLSTATE: 42825; 12830)
[2024-05-13T04:59:02.348+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-05-13T04:59:02.372+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-05-13T04:59:02.375+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
