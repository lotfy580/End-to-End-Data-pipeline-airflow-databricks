[2024-05-13T04:58:29.650+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-13T04:58:29.676+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: customer_transactions_pipeline.Presentation_layer_PySpark.dim_transaction_details_transformation manual__2024-05-13T04:55:09.068546+00:00 [queued]>
[2024-05-13T04:58:29.686+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: customer_transactions_pipeline.Presentation_layer_PySpark.dim_transaction_details_transformation manual__2024-05-13T04:55:09.068546+00:00 [queued]>
[2024-05-13T04:58:29.687+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2024-05-13T04:58:29.702+0000] {taskinstance.py:2327} INFO - Executing <Task(DatabricksRunNowOperator): Presentation_layer_PySpark.dim_transaction_details_transformation> on 2024-05-13 04:55:09.068546+00:00
[2024-05-13T04:58:29.706+0000] {standard_task_runner.py:63} INFO - Started process 12828 to run task
[2024-05-13T04:58:29.710+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'customer_transactions_pipeline', 'Presentation_layer_PySpark.dim_transaction_details_transformation', 'manual__2024-05-13T04:55:09.068546+00:00', '--job-id', '100', '--raw', '--subdir', 'DAGS_FOLDER/dag.py', '--cfg-path', '/tmp/tmpuci4n7ql']
[2024-05-13T04:58:29.712+0000] {standard_task_runner.py:91} INFO - Job 100: Subtask Presentation_layer_PySpark.dim_transaction_details_transformation
[2024-05-13T04:58:29.758+0000] {task_command.py:426} INFO - Running <TaskInstance: customer_transactions_pipeline.Presentation_layer_PySpark.dim_transaction_details_transformation manual__2024-05-13T04:55:09.068546+00:00 [running]> on host f5da772b0226
[2024-05-13T04:58:29.845+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='mm22@gmail.com' AIRFLOW_CTX_DAG_OWNER='ct' AIRFLOW_CTX_DAG_ID='customer_transactions_pipeline' AIRFLOW_CTX_TASK_ID='Presentation_layer_PySpark.dim_transaction_details_transformation' AIRFLOW_CTX_EXECUTION_DATE='2024-05-13T04:55:09.068546+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-05-13T04:55:09.068546+00:00'
[2024-05-13T04:58:29.846+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-13T04:58:29.868+0000] {base.py:84} INFO - Using connection ID 'databricks_default' for task execution.
[2024-05-13T04:58:29.870+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:58:30.452+0000] {databricks.py:56} INFO - Run submitted with run_id: 981708475230361
[2024-05-13T04:58:30.452+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:58:30.991+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:58:31.466+0000] {databricks.py:109} INFO - Presentation_layer_PySpark.dim_transaction_details_transformation in run state: {'life_cycle_state': 'QUEUED', 'result_state': '', 'state_message': ''}
[2024-05-13T04:58:31.468+0000] {databricks.py:110} INFO - View run status, Spark UI, and logs at https://adb-321042217431130.10.azuredatabricks.net/?o=321042217431130#job/848711502692621/run/981708475230361
[2024-05-13T04:58:31.468+0000] {databricks.py:111} INFO - Sleeping for 30 seconds.
[2024-05-13T04:59:01.472+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:59:01.777+0000] {databricks_base.py:501} INFO - Using token auth.
[2024-05-13T04:59:02.228+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-13T04:59:02.258+0000] {taskinstance.py:2890} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/databricks/operators/databricks.py", line 858, in execute
    _handle_databricks_operator_execution(self, hook, self.log, context)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/databricks/operators/databricks.py", line 107, in _handle_databricks_operator_execution
    raise AirflowException(error_message)
airflow.exceptions.AirflowException: Presentation_layer_PySpark.dim_transaction_details_transformation failed with terminal state: {'life_cycle_state': 'INTERNAL_ERROR', 'result_state': 'FAILED', 'state_message': 'Task dim_transaction_details_transformation failed with message: Workload failed, see run output for details. This caused all downstream tasks to get skipped.'} and with the error [DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same
target row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,
when multiple source rows match on the same target row, the result may be ambiguous
as it is unclear which source row should be used to update or delete the matching
target row. You can preprocess the source table to eliminate the possibility of
multiple matches. Please refer to
https://docs.microsoft.com/azure/databricks/delta/merge#merge-error SQLSTATE: 21506
[2024-05-13T04:59:02.272+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=customer_transactions_pipeline, task_id=Presentation_layer_PySpark.dim_transaction_details_transformation, execution_date=20240513T045509, start_date=20240513T045829, end_date=20240513T045902
[2024-05-13T04:59:02.294+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 100 for task Presentation_layer_PySpark.dim_transaction_details_transformation (Presentation_layer_PySpark.dim_transaction_details_transformation failed with terminal state: {'life_cycle_state': 'INTERNAL_ERROR', 'result_state': 'FAILED', 'state_message': 'Task dim_transaction_details_transformation failed with message: Workload failed, see run output for details. This caused all downstream tasks to get skipped.'} and with the error [DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same
target row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,
when multiple source rows match on the same target row, the result may be ambiguous
as it is unclear which source row should be used to update or delete the matching
target row. You can preprocess the source table to eliminate the possibility of
multiple matches. Please refer to
https://docs.microsoft.com/azure/databricks/delta/merge#merge-error SQLSTATE: 21506; 12828)
[2024-05-13T04:59:02.306+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-05-13T04:59:02.332+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-05-13T04:59:02.335+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
